import os
import re
import pymysql
import numpy as np
import pandas as pd
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from joblib import load as joblib_load
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from crawler import crawl_and_return  # pastikan file crawler.py ada
import asyncio
import nltk
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

nltk.download('stopwords')

# ── Load ENV ───────────────────────────────────────────────────────────────
load_dotenv()
DB_HOST     = os.getenv("DB_HOST", "localhost")
DB_USER     = os.getenv("DB_USER", "root")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")
DB_NAME     = os.getenv("DB_NAME", "sentiment_db")

# ── Inisialisasi App ───────────────────────────────────────────────────────
app = Flask(__name__)
CORS(app)

# ── Koneksi Database ───────────────────────────────────────────────────────
def get_db_connection():
    return pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db=DB_NAME)

# ── Load Model dan Vectorizer ──────────────────────────────────────────────
count_vectorizer = joblib_load("count_vectorizer.pkl")
nb_model         = joblib_load("naive_bayes_model.pkl")
svm_model        = joblib_load("svm_model.pkl")

# ── Load IndoBERT ──────────────────────────────────────────────────────────
MODEL_PATH = "my-finetuned-bert"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
bert_model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
bert_pipeline = pipeline("text-classification", model=bert_model, tokenizer=tokenizer)

# ── Keyword Risiko ─────────────────────────────────────────────────────────
RISK_KEYWORDS = ["slot", "pinjol", "gacor", "jp", "depo", "spin"]

# ── Preprocessing Tools ────────────────────────────────────────────────────
kamus_data = pd.read_excel('kamuskatabaku.xlsx')
kamus_tidak_baku = dict(zip(kamus_data['tidak_baku'], kamus_data['kata_baku']))
stop_words = set(stopwords.words('indonesian'))
stemmer = StemmerFactory().create_stemmer()

def clean_text(text):
    if not isinstance(text, str):
        return ''
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'@[\w_]+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    return text

def normalize_text(text):
    return ' '.join([kamus_tidak_baku.get(word, word) for word in text.split()])

def remove_stopwords(tokens):
    return [word for word in tokens if word not in stop_words]

def stem_words(tokens):
    return ' '.join([stemmer.stem(word) for word in tokens])

def detect_risk(text):
    return any(k in text for k in RISK_KEYWORDS)

# ── Endpoint: Preprocessing ────────────────────────────────────────────────
@app.route("/preprocess", methods=["POST"])
def preprocess_comments():
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT id, text FROM comments")
        rows = cursor.fetchall()

        for row in rows:
            comment_id, original_text = row
            cleaned = clean_text(original_text)
            normalized = normalize_text(cleaned)
            tokens = normalized.split()
            filtered = remove_stopwords(tokens)
            stemmed = stem_words(filtered)

            cursor.execute("UPDATE comments SET cleaning = %s WHERE id = %s", (stemmed, comment_id))

        conn.commit()
        cursor.close()
        conn.close()
        return jsonify({
            "status": "success",
            "message": "Preprocessing selesai dan disimpan ke kolom cleaning",
            "total_processed": len(rows)
        })

    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500


# ── Endpoint: Crawl Tweet ──────────────────────────────────────────────────
@app.route("/crawl-tweets", methods=["POST"])
def crawl_tweets():
    data = request.get_json() or {}
    jumlah = int(data.get("jumlah", 50))
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        tweets = loop.run_until_complete(crawl_and_return(jumlah))

        inserted = 0
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                for tweet in tweets:
                    teks = tweet["text"]
                    cleaned = clean_text(teks)
                    tfidf = count_vectorizer.transform([cleaned])
                    pred_nb = nb_model.predict(tfidf)[0]
                    sentiment = "JUDOL" if str(pred_nb) == "1" else "Bukan JUDOL"

                    cur.execute("""
                        INSERT INTO comments (text, sentiment, created_at)
                        VALUES (%s, %s, %s)
                    """, (teks, sentiment, tweet["created_at"]))
                    inserted += 1
                conn.commit()

        return jsonify({"status": "success", "inserted": inserted})

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ── Endpoint: Prediksi Sentimen ────────────────────────────────────────────
@app.route("/predict", methods=["POST"])
def predict_sentiment():
    data = request.get_json() or {}
    comment = data.get("komentar")

    if not comment or not isinstance(comment, str):
        return jsonify({"error": "Komentar tidak valid"}), 400

    cleaned = clean_text(comment)
    try:
        X_tfidf = count_vectorizer.transform([cleaned])
        pred_nb = nb_model.predict(X_tfidf)[0]
        conf_nb = nb_model.predict_proba(X_tfidf).max()

        pred_svm = svm_model.predict(X_tfidf)[0]
        if hasattr(svm_model, "predict_proba"):
            conf_svm = svm_model.predict_proba(X_tfidf).max()
        else:
            score = svm_model.decision_function(X_tfidf)[0]
            conf_svm = float(1 / (1 + np.exp(-score)))

        bert_result = bert_pipeline(cleaned)[0]
        pred_bert = bert_result["label"]
        conf_bert = float(bert_result["score"])

        label_map = {"LABEL_0": "Bukan JUDOL", "LABEL_1": "JUDOL", 0: "Bukan JUDOL", 1: "JUDOL"}

        return jsonify({
            "input": comment,
            "naive_bayes": {
                "sentiment": label_map.get(pred_nb, str(pred_nb)),
                "confidence": round(conf_nb, 4)
            },
            "svm": {
                "sentiment": label_map.get(pred_svm, str(pred_svm)),
                "confidence": round(conf_svm, 4)
            },
            "indobert": {
                "sentiment": label_map.get(pred_bert, pred_bert),
                "confidence": round(conf_bert, 4)
            },
            "risk_flag": True if str(pred_nb) == "1" or pred_bert == "LABEL_1" else detect_risk(cleaned)
        }), 200

    except Exception as e:
        return jsonify({"error": f"Model error: {str(e)}"}), 500

# ── Endpoint: Get Comments ─────────────────────────────────────────────────
@app.route("/get-comments", methods=["GET"])
def get_comments():
    with get_db_connection() as conn:
        with conn.cursor(pymysql.cursors.DictCursor) as cur:
            cur.execute("SELECT id, text, cleaning, sentiment, created_at FROM comments ORDER BY created_at DESC LIMIT 100")
            rows = cur.fetchall()
    return jsonify(rows)

# ── Endpoint: Delete Comment ───────────────────────────────────────────────
@app.route('/delete-comment/<int:comment_id>', methods=['DELETE'])
def delete_comment(comment_id):
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM comments WHERE id = %s", (comment_id,))
        row = cursor.fetchone()
        if not row:
            return jsonify({"status": "error", "error": "ID tidak ditemukan"}), 404

        cursor.execute("DELETE FROM comments WHERE id = %s", (comment_id,))
        conn.commit()
        cursor.close()
        conn.close()
        return jsonify({"status": "success", "deleted_id": comment_id})

    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

# ── Endpoint: Deteksi Risiko ───────────────────────────────────────────────
@app.route("/detect-risk", methods=["POST"])
def risk_only():
    data = request.get_json() or {}
    comment = data.get("komentar", "")
    cleaned = clean_text(comment)
    return jsonify({"risk_flag": detect_risk(cleaned)})

# ── Run Server ─────────────────────────────────────────────────────────────
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
II

import os
import re
import pymysql
import numpy as np
import pandas as pd
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from joblib import load as joblib_load
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from crawler import crawl_and_return
import asyncio
import nltk
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

nltk.download('stopwords')

# Load .env
load_dotenv()
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_USER = os.getenv("DB_USER", "root")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")
DB_NAME = os.getenv("DB_NAME", "sentiment_db")

# App
app = Flask(__name__)
CORS(app)

# Database Connection
def get_db_connection():
    return pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db=DB_NAME)

# Load Models
tfidf_vectorizer = joblib_load("tfidf_vectorizer.joblib")
nb_model = joblib_load("naive_bayes_model.joblib")
svm_model = joblib_load("svm_model.joblib")

# Load IndoBERT
MODEL_PATH = "my-finetuned-bert"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
bert_model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
bert_pipeline = pipeline("text-classification", model=bert_model, tokenizer=tokenizer)

# Risiko Keywords
RISK_KEYWORDS = ["slot", "judol", "ngeslot", "pinjol", "gacor", "jp", "depo", "spin", "wd", "jackpot", "togel"]

# Preprocessing Tools
kamus_data = pd.read_excel('kamuskatabaku.xlsx')
kamus_tidak_baku = dict(zip(kamus_data['tidak_baku'], kamus_data['kata_baku']))
stop_words = set(stopwords.words('indonesian'))
stemmer = StemmerFactory().create_stemmer()

def clean_text(text):
    if not isinstance(text, str):
        return ''
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'@[\w_]+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    return text

def normalize_text(text):
    return ' '.join([kamus_tidak_baku.get(word, word) for word in text.split()])

def remove_stopwords(tokens):
    return [word for word in tokens if word not in stop_words]

def stem_words(tokens):
    return ' '.join([stemmer.stem(word) for word in tokens])

def detect_risk(text):
    """
    Mengembalikan True jika salah satu kata di RISK_KEYWORDS muncul di dalam teks.
    Pastikan teks sudah di‐lowercase sebelum cek.
    """
    return any(k in text for k in RISK_KEYWORDS)

# ──────────────────────────────────────────────────────────────────────────────
# Crawl Tweet Endpoint
@app.route("/crawl-tweets", methods=["POST"])
def crawl_tweets():
    data = request.get_json() or {}
    jumlah = int(data.get("jumlah", 50))
    try:
        # Ambil tweets (cuma text + created_at)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        tweets = loop.run_until_complete(crawl_and_return(jumlah))

        inserted = 0
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                for tweet in tweets:
                    teks    = tweet["text"]
                    created = tweet["created_at"]

                    # 1) Hitung risk_flag berdasarkan teks mentah (atau bisa juga pake clean_text)
                    cleaned_for_risk = clean_text(teks)
                    is_risky = 1 if detect_risk(cleaned_for_risk) else 0

                    # 2) Lakukan INSERT, termasuk kolom risk_flag
                    cur.execute("""
                        INSERT INTO comments (text, created_at, risk_flag)
                        VALUES (%s, %s, %s)
                    """, (teks, created, is_risky))
                    inserted += 1

                conn.commit()

        return jsonify({"status": "success", "inserted": inserted}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ──────────────────────────────────────────────────────────────────────────────
# Preprocess + Klasifikasi SVM-only (Overwrite sentiment)
@app.route("/preprocess", methods=["POST"])
def preprocess_svm_only_overwrite():
    try:
        conn = get_db_connection()
        cursor = conn.cursor(pymysql.cursors.DictCursor)

        # Ambil baris yang belum diproses SVM (sentiment IS NULL atau 'unknown')
        cursor.execute("""
            SELECT id, text 
              FROM comments 
             WHERE sentiment IS NULL 
                OR sentiment = 'unknown'
        """)
        rows = cursor.fetchall()

        if not rows:
            cursor.close()
            conn.close()
            return jsonify({
                "status": "success",
                "total_processed": 0
            }), 200

        total = 0
        for row in rows:
            cid   = row["id"]
            text  = row["text"]

            # ——— 1) PREPROCESSING ———
            cleaned_basic = clean_text(text)
            normalized    = normalize_text(cleaned_basic)
            tokens        = normalized.split()
            filtered      = remove_stopwords(tokens)
            stemmed       = stem_words(filtered)

            # ——— 2) TF-IDF untuk SVM ———
            tfidf_vec = tfidf_vectorizer.transform([cleaned_basic])
            X_dense   = tfidf_vec.toarray()

            # ——— 3) Prediksi SVM ———
            if hasattr(svm_model, "predict_proba"):
                probs = svm_model.predict_proba(X_dense)[0]
                raw_pred = np.argmax(probs)
                conf     = float(probs.max())
            else:
                raw_pred = svm_model.predict(X_dense)[0]
                decision = svm_model.decision_function(X_dense)[0]
                if isinstance(decision, np.ndarray):
                    score = max(decision)
                else:
                    score = decision
                conf = float(1.0 / (1.0 + np.exp(-score)))

            # ——— 4) Mapping raw_pred ke label string ———
            if isinstance(raw_pred, int):
                label_map = {0: "negatif", 1: "netral", 2: "positif"}
                label_svm = label_map.get(raw_pred, "unknown")
            else:
                # Jika raw_pred berupa string (misalnya "negatif"), pakai langsung
                label_svm = raw_pred

            if not label_svm:
                label_svm = "unknown"

            # ——— 5) Hitung risk_flag berdasarkan teks yang sudah dibersihkan ———
            # Kita bisa cek pada cleaned_basic (tanpa tanda baca, digit, dll.)
            is_risky = 1 if detect_risk(cleaned_basic) else 0

            # ——— 6) UPDATE database (cleaning, sentiment, confidence, risk_flag) ———
            cursor.execute("""
                UPDATE comments
                   SET cleaning    = %s,
                       sentiment   = %s,
                       confidence  = %s,
                       risk_flag   = %s
                 WHERE id = %s
            """, (stemmed, label_svm, conf, is_risky, cid))

            total += 1

        conn.commit()
        cursor.close()
        conn.close()

        return jsonify({
            "status": "success",
            "total_processed": total
        }), 200

    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

# ──────────────────────────────────────────────────────────────────────────────
# Predict Endpoint (Memo: tetap menampilkan risk_flag juga dari input API)
@app.route("/predict", methods=["POST"])
def predict_sentiment():
    data = request.get_json() or {}
    comment = data.get("komentar")

    if not comment or not isinstance(comment, str):
        return jsonify({"error": "Komentar tidak valid"}), 400

    cleaned = clean_text(comment)
    try:
        X_tfidf = tfidf_vectorizer.transform([cleaned])
        label_map = {0: "negatif", 1: "netral", 2: "positif"}

        # Naive Bayes
        pred_nb = nb_model.predict(X_tfidf)[0]
        conf_nb = float(nb_model.predict_proba(X_tfidf).max())

        # SVM - convert to dense
        X_dense = X_tfidf.toarray()
        pred_svm = svm_model.predict(X_dense)[0]
        if hasattr(svm_model, "predict_proba"):
            conf_svm = float(svm_model.predict_proba(X_dense).max())
        else:
            decision = svm_model.decision_function(X_dense)[0]
            if isinstance(decision, np.ndarray):
                score = max(decision)
            else:
                score = decision
            conf_svm = float(1 / (1 + np.exp(-score)))

        # IndoBERT
        bert_result = bert_pipeline(cleaned)[0]
        bert_label = bert_result["label"]
        bert_score = float(bert_result["score"])
        bert_label_map = {
            "LABEL_0": "negatif",
            "LABEL_1": "netral",
            "LABEL_2": "positif"
        }
        bert_sentiment = bert_label_map.get(bert_label, bert_label)
        if bert_score < 0.58:
            bert_sentiment = "netral"

        # Risk flag untuk komentar baru ini
        risk_flag_val = 1 if detect_risk(cleaned) else 0

        return jsonify({
            "input": comment,
            "naive_bayes": {
                "sentiment": label_map.get(pred_nb, str(pred_nb)),
                "confidence": round(conf_nb, 4)
            },
            "svm": {
                "sentiment": label_map.get(pred_svm, str(pred_svm)),
                "confidence": round(conf_svm, 4)
            },
            "indobert": {
                "sentiment": bert_sentiment,
                "confidence": round(bert_score, 4)
            },
            "risk_flag": risk_flag_val
        }), 200

    except Exception as e:
        return jsonify({"error": f"Model error: {str(e)}"}), 500

# ──────────────────────────────────────────────────────────────────────────────
# Get Comments (termasuk risk_flag)
@app.route("/get-comments", methods=["GET"])
def get_comments():
    with get_db_connection() as conn:
        with conn.cursor(pymysql.cursors.DictCursor) as cur:
            # Tambahkan kolom risk_flag di SELECT supaya frontend bisa menampilkannya
            cur.execute("""
                SELECT id,
                       text,
                       cleaning,
                       sentiment,
                       confidence,
                       risk_flag,
                       created_at
                  FROM comments
              ORDER BY created_at DESC
                 LIMIT 100
            """)
            rows = cur.fetchall()
    return jsonify(rows)

# ──────────────────────────────────────────────────────────────────────────────
# Delete Comment
@app.route('/delete-comment/<int:comment_id>', methods=['DELETE'])
def delete_comment(comment_id):
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM comments WHERE id = %s", (comment_id,))
        row = cursor.fetchone()
        if not row:
            return jsonify({"status": "error", "error": "ID tidak ditemukan"}), 404

        cursor.execute("DELETE FROM comments WHERE id = %s", (comment_id,))
        conn.commit()
        cursor.close()
        conn.close()
        return jsonify({"status": "success", "deleted_id": comment_id})

    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

# ──────────────────────────────────────────────────────────────────────────────
# Risk Detection Only
@app.route("/detect-risk", methods=["POST"])
def risk_only():
    data = request.get_json() or {}
    comment = data.get("komentar", "")
    cleaned = clean_text(comment)
    return jsonify({"risk_flag": 1 if detect_risk(cleaned) else 0})

# ──────────────────────────────────────────────────────────────────────────────
@app.route('/deleteall-comments', methods=['DELETE'])
def delete_all_comments():
    try:
        conn = get_db_connection()
        with conn.cursor() as cursor:
            cursor.execute("DELETE FROM comments")
            deleted = cursor.rowcount
            conn.commit()
        conn.close()
        return jsonify({
            "status": "success",
            "message": f"{deleted} komentar berhasil dihapus."
        }), 200
    except Exception as e:
        return jsonify({
            "status": "error",
            "error": str(e)
        }), 500

# Run Server
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
