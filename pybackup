; # import os
; # import re
; # from flask import Flask, request, jsonify
; # import pymysql
; # from dotenv import load_dotenv
; # from joblib import load as joblib_load

; # # ── Load ENV ───────────────────────────────────────────────────────────────
; # load_dotenv()
; # DB_HOST     = os.getenv("DB_HOST", "localhost")
; # DB_USER     = os.getenv("DB_USER", "root")
; # DB_PASSWORD = os.getenv("DB_PASSWORD", "")
; # DB_NAME     = os.getenv("DB_NAME", "sentiment_db")

; # # ── Inisialisasi App ───────────────────────────────────────────────────────
; # app = Flask(__name__)

; # # ── Muat Model & Vectorizer via joblib ─────────────────────────────────────
; # vectorizer = joblib_load("count_vectorizer.pkl")
; # model      = joblib_load("naive_bayes_model.pkl")

; # # ── Kata Kunci Risiko ─────────────────────────────────────────────────────
; # RISK_KEYWORDS = ["slot", "pinjol", "gacor", "jp", "depo", "spin"]

; # def clean_text(text: str) -> str:
; #     text = text.lower()
; #     text = re.sub(r'[^a-z\s]', '', text)
; #     return text

; # def detect_risk(text: str) -> bool:
; #     return any(k in text for k in RISK_KEYWORDS)

; # @app.route("/predict", methods=["POST"])
; # def predict_sentiment():
; #     data = request.get_json() or {}
; #     comment = data.get("komentar")

; #     if not comment or not isinstance(comment, str):
; #         return jsonify({"error": "Komentar tidak valid"}), 400

; #     cleaned = clean_text(comment)

; #     try:
; #         X = vectorizer.transform([cleaned])
; #         pred_label = model.predict(X)[0]

; #         confidence = None
; #         if hasattr(model, "predict_proba"):
; #             confidence = float(model.predict_proba(X).max())

; #         risk_flag = detect_risk(cleaned)

; #         return jsonify({
; #             "sentiment": str(pred_label),
; #             "confidence": confidence,
; #             "risk_flag": True if str(pred_label) == "1" else detect_risk(cleaned)
; #         }), 200

; #     except Exception as e:
; #         return jsonify({"error": f"Model error: {str(e)}"}), 500


; # @app.route("/get-comments", methods=["GET"])
; # def get_comments():
; #     with pymysql.connect(
; #         host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db=DB_NAME
; #     ) as conn:
; #         with conn.cursor(pymysql.cursors.DictCursor) as cur:
; #             cur.execute("""
; #                 SELECT id, text, sentiment, created_at
; #                 FROM comments
; #                 ORDER BY created_at DESC
; #                 LIMIT 100
; #             """)
; #             rows = cur.fetchall()
; #     return jsonify(rows)


; # @app.route("/detect-risk", methods=["POST"])
; # def risk_only():
; #     data    = request.get_json() or {}
; #     comment = data.get("komentar", "")
; #     cleaned = clean_text(comment)
; #     return jsonify({"risk_flag": detect_risk(cleaned)})

; # if __name__ == "__main__":
; #     app.run(host="0.0.0.0", port=5000, debug=True)

; # import os
; # import re
; # from flask import Flask, request, jsonify
; # import pymysql
; # from dotenv import load_dotenv
; # from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

; # # ── Load ENV ───────────────────────────────────────────────────────────────
; # load_dotenv()
; # DB_HOST     = os.getenv("DB_HOST", "localhost")
; # DB_USER     = os.getenv("DB_USER", "root")
; # DB_PASSWORD = os.getenv("DB_PASSWORD", "")
; # DB_NAME     = os.getenv("DB_NAME", "sentiment_db")

; # # ── Inisialisasi App ───────────────────────────────────────────────────────
; # app = Flask(__name__)

; # # ── Muat Model & Tokenizer (Huggingface) ───────────────────────────────────
; # MODEL_PATH = "my-finetuned-bert"  # Folder hasil trainer.save_model()
; # tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
; # model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
; # classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)

; # # ── Kata Kunci Risiko ─────────────────────────────────────────────────────
; # RISK_KEYWORDS = ["slot", "pinjol", "gacor", "jp", "depo", "spin"]

; # def clean_text(text: str) -> str:
; #     text = text.lower()
; #     text = re.sub(r'[^a-z\s]', '', text)
; #     return text

; # def detect_risk(text: str) -> bool:
; #     return any(k in text for k in RISK_KEYWORDS)

; # @app.route("/predict", methods=["POST"])
; # def predict_sentiment():
; #     data = request.get_json() or {}
; #     comment = data.get("komentar")

; #     if not comment or not isinstance(comment, str):
; #         return jsonify({"error": "Komentar tidak valid"}), 400

; #     cleaned = clean_text(comment)

; #     try:
; #         result = classifier(cleaned)[0]
; #         label = result["label"]  # "LABEL_0" atau "LABEL_1"
; #         score = float(result["score"])

; #         # Mapping label jika mau ditampilkan sebagai string
; #         label_map = {
; #             "LABEL_0": "Bukan JUDOL",
; #             "LABEL_1": "JUDOL"
; #         }

; #         return jsonify({
; #             "sentiment": label_map.get(label, label),
; #             "confidence": round(score, 4),
; #             "risk_flag": True if label == "LABEL_1" else detect_risk(cleaned)
; #         }), 200

; #     except Exception as e:
; #         return jsonify({"error": f"Model error: {str(e)}"}), 500

; # @app.route("/get-comments", methods=["GET"])
; # def get_comments():
; #     with pymysql.connect(
; #         host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db=DB_NAME
; #     ) as conn:
; #         with conn.cursor(pymysql.cursors.DictCursor) as cur:
; #             cur.execute("""
; #                 SELECT id, text, sentiment, created_at
; #                 FROM comments
; #                 ORDER BY created_at DESC
; #                 LIMIT 100
; #             """)
; #             rows = cur.fetchall()
; #     return jsonify(rows)

; # @app.route("/detect-risk", methods=["POST"])
; # def risk_only():
; #     data = request.get_json() or {}
; #     comment = data.get("komentar", "")
; #     cleaned = clean_text(comment)
; #     return jsonify({"risk_flag": detect_risk(cleaned)})

; # if __name__ == "__main__":
; #     app.run(host="0.0.0.0", port=5000, debug=True)

; # async def crawl_and_return(limit=500):
;     config = ConfigParser()
;     config.read('config.ini')
;     username = config['X']['username']
;     email = config['X']['email']
;     password = config['X']['password']

;     client = Client()
;     client.load_cookies('cookies.json')

;     tweet_count = 0
;     tweets = None
;     results = []

;     QUERY = '(judi OR slot OR gacor OR togel) lang:id -is:retweet -filter:links -filter:media'

;     while tweet_count < limit:
;         try:
;             if tweets is None:
;                 tweets = await client.search_tweet(QUERY, product='Top')
;             else:
;                 await asyncio.sleep(randint(3, 7))
;                 tweets = await tweets.next()

;             if not tweets:
;                 break

;             for tweet in tweets:
;                 tweet_count += 1
;                 results.append({
;                     "text": tweet.text,
;                     "user": tweet.user.name,
;                     "created_at": tweet.created_at.strftime("%Y-%m-%d %H:%M:%S")
;                 })
;                 if tweet_count >= limit:
;                     break

;         except TooManyRequests as e:
;             await asyncio.sleep((datetime.fromtimestamp(e.rate_limit_reset) - datetime.now()).total_seconds())

;     return results
; #     # 1) Baca kredensial
; #     config = ConfigParser()
; #     config.read('config.ini')
; #     username = config['X']['username']
; #     email    = config['X']['email']
; #     password = config['X']['password']

; #     client = Client()
; #     # 2) Inisialisasi client & login (async)
; #     # client = Client(language='en-US')
; #     # await client.login(
; #     #     auth_info_1=username,
; #     #     auth_info_2=email,
; #     #     password=password
; #     # )

; #     # 3) Synchronously simpan cookies
; #     # client.save_cookies('cookies.json')
; #     client.load_cookies('cookies.json')
; # #     client.load_cookies({
; # #     "auth_token": "b5d208a100e347c82d5654c91ea3dfe89bd7329d",
; # #     "ct0": "96e7fe78ff622544ef3af4b8428bdd92dd113eb0d56efff4a493c433c033c0ae14e12100724d7546adad467efcc2802ca6eac74d398a37c2c88d7d10c49e12c7623777f2f3a69fed9738e602eb31ada8",
; # #     "twid": "u%3D1554484215315824640"
; # # })

; #     # 4) Siapkan CSV
; #     with open('tweets2.csv', 'w', newline='', encoding='utf-8') as f:
; #         writer = csv.writer(f)
; #         writer.writerow([
; #             'Tweet_count', 'Username', 'Text',
; #             'Created At', 'Retweets', 'Likes'
; #         ])

; #     # 5) Loop pengambilan tweet
; #     tweet_count = 0
; #     tweets = None

; #     while tweet_count < MINIMUM_TWEETS:
; #         try:
; #             if tweets is None:
; #                 print(f'{datetime.now()} – Mendapatkan batch pertama…')
; #                 tweets = await client.search_tweet(QUERY, product='Top')
; #             else:
; #                 wait_time = randint(5, 10)
; #                 print(f'{datetime.now()} – Tunggu {wait_time}s sebelum batch selanjutnya…')
; #                 await asyncio.sleep(wait_time)
; #                 tweets = await tweets.next()

; #             if not tweets:
; #                 print(f'{datetime.now()} – Tidak ada tweet lagi.')
; #                 break

; #             for tweet in tweets:
; #                 tweet_count += 1
; #                 row = [
; #                     tweet_count,
; #                     tweet.user.name,
; #                     tweet.text,
; #                     tweet.created_at,
; #                     tweet.retweet_count,
; #                     tweet.favorite_count
; #                 ]
; #                 with open('tweets2.csv', 'a', newline='', encoding='utf-8') as f:
; #                     csv.writer(f).writerow(row)

; #             print(f'{datetime.now()} – Total terambil: {tweet_count} tweets')

; #         except TooManyRequests as e:
; #             reset_time = datetime.fromtimestamp(e.rate_limit_reset)
; #             print(f'{datetime.now()} – Rate limit. Tunggu hingga {reset_time}')
; #             delay = (reset_time - datetime.now()).total_seconds()
; #             if delay > 0:
; #                 await asyncio.sleep(delay)

; #     print(f'{datetime.now()} – Selesai! Mendapatkan {tweet_count} tweets.')

; # if __name__ == "__main__":
; #     asyncio.run(main())

; import os
; import re
; from flask import Flask, request, jsonify
; import pymysql
; import numpy as np
; from dotenv import load_dotenv
; from joblib import load as joblib_load
; from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
; from crawler import crawl_and_return  # pastikan file crawler.py ada
; import asyncio
; from flask_cors import CORS

; # ── Load ENV ───────────────────────────────────────────────────────────────
; load_dotenv()
; DB_HOST     = os.getenv("DB_HOST", "localhost")
; DB_USER     = os.getenv("DB_USER", "root")
; DB_PASSWORD = os.getenv("DB_PASSWORD", "")
; DB_NAME     = os.getenv("DB_NAME", "sentiment_db")

; def get_db_connection():
;         return pymysql.connect(
;             host=DB_HOST,
;             user=DB_USER,
;             password=DB_PASSWORD,
;             db=DB_NAME
;         )

; # ── Inisialisasi App ───────────────────────────────────────────────────────
; app = Flask(__name__)
; CORS(app)

; # ── Muat Model & Vectorizer ────────────────────────────────────────────────
; count_vectorizer = joblib_load("count_vectorizer.pkl")
; nb_model         = joblib_load("naive_bayes_model.pkl")
; svm_model        = joblib_load("svm_model.pkl")  # Pastikan sudah disimpan sebelumnya

; # ── Muat Model IndoBERT ────────────────────────────────────────────────────
; MODEL_PATH = "my-finetuned-bert"
; tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
; bert_model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
; bert_pipeline = pipeline("text-classification", model=bert_model, tokenizer=tokenizer)

; # ── Kata Kunci Risiko ─────────────────────────────────────────────────────
; RISK_KEYWORDS = ["slot", "pinjol", "gacor", "jp", "depo", "spin"]

; @app.route("/crawl-tweets", methods=["POST"])
; def crawl_tweets():
;     data = request.get_json() or {}
;     jumlah = int(data.get("jumlah", 50))

;     try:
;         loop = asyncio.new_event_loop()
;         asyncio.set_event_loop(loop)
;         tweets = loop.run_until_complete(crawl_and_return(jumlah))

;         inserted = 0
;         with pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db=DB_NAME) as conn:
;             with conn.cursor() as cur:
;                 for tweet in tweets:
;                     teks = tweet["text"]
;                     cleaned = clean_text(teks)
;                     tfidf = count_vectorizer.transform([cleaned])
;                     pred_nb = nb_model.predict(tfidf)[0]
;                     sentiment = "JUDOL" if str(pred_nb) == "1" else "Bukan JUDOL"

;                     cur.execute("""
;                         INSERT INTO comments (text, sentiment, created_at)
;                         VALUES (%s, %s, %s)
;                     """, (teks, sentiment, tweet["created_at"]))
;                     inserted += 1
;                 conn.commit()

;         return jsonify({
;             "status": "success",
;             "inserted": inserted
;         })

;     except Exception as e:
;         return jsonify({"error": str(e)}), 500

; def clean_text(text: str) -> str:
;     text = text.lower()
;     text = re.sub(r'[^a-z\s]', '', text)
;     return text

; def detect_risk(text: str) -> bool:
;     return any(k in text for k in RISK_KEYWORDS)

; @app.route("/predict", methods=["POST"])
; def predict_sentiment():
;     data = request.get_json() or {}
;     comment = data.get("komentar")

;     if not comment or not isinstance(comment, str):
;         return jsonify({"error": "Komentar tidak valid"}), 400

;     cleaned = clean_text(comment)
;     try:
;         # ── TF-IDF transform
;         X_tfidf = count_vectorizer.transform([cleaned])

;         # ── Prediksi Naive Bayes
;         pred_nb = nb_model.predict(X_tfidf)[0]
;         conf_nb = nb_model.predict_proba(X_tfidf).max()

;         # ── Prediksi SVM
;         pred_svm = svm_model.predict(X_tfidf)[0]
;         if hasattr(svm_model, "predict_proba"):
;             # jika kamu training SVC(probability=True)
;             conf_svm = svm_model.predict_proba(X_tfidf).max()
;         else:
;             # fallback: pakai margin decision_function + sigmoid agar jadi [0,1]
;             score = svm_model.decision_function(X_tfidf)[0]
;             conf_svm = float(1 / (1 + np.exp(-score)))
;         # ── Prediksi IndoBERT
;         bert_result = bert_pipeline(cleaned)[0]
;         pred_bert = bert_result["label"]  # LABEL_0 atau LABEL_1
;         conf_bert = float(bert_result["score"])

;         label_map = {
;             "LABEL_0": "Bukan JUDOL",
;             "LABEL_1": "JUDOL",
;             0: "Bukan JUDOL",
;             1: "JUDOL"
;         }

;         return jsonify({
;             "input": comment,
;             "naive_bayes": {
;                 "sentiment": label_map.get(pred_nb, str(pred_nb)),
;                 "confidence": round(conf_nb, 4)
;             },
;             "svm": {
;                 "sentiment": label_map.get(pred_svm, str(pred_svm)),
;                 "confidence": round(conf_svm, 4)
;             },
;             "indobert": {
;                 "sentiment": label_map.get(pred_bert, pred_bert),
;                 "confidence": round(conf_bert, 4)
;             },
;             "risk_flag": True if str(pred_nb) == "1" or pred_bert == "LABEL_1" else detect_risk(cleaned)
;         }), 200

;     except Exception as e:
;         return jsonify({"error": f"Model error: {str(e)}"}), 500

; @app.route("/get-comments", methods=["GET"])
; def get_comments():
;     with pymysql.connect(
;         host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db=DB_NAME
;     ) as conn:
;         with conn.cursor(pymysql.cursors.DictCursor) as cur:
;             cur.execute("""
;                 SELECT id, text, sentiment, created_at
;                 FROM comments
;                 ORDER BY created_at DESC
;                 LIMIT 100
;             """)
;             rows = cur.fetchall()
;     return jsonify(rows)

; @app.route('/delete-comment/<int:comment_id>', methods=['DELETE'])
; def delete_comment(comment_id):
;     try:
;         # Koneksi ke database
;         conn = get_db_connection()
;         cursor = conn.cursor()

;         # Cek apakah ID komentar ada
;         cursor.execute("SELECT * FROM comments WHERE id = %s", (comment_id,))
;         row = cursor.fetchone()
;         if not row:
;             return jsonify({"status": "error", "error": "ID tidak ditemukan"}), 404

;         # Hapus komentar
;         cursor.execute("DELETE FROM comments WHERE id = %s", (comment_id,))
;         conn.commit()
;         cursor.close()
;         conn.close()

;         return jsonify({"status": "success", "deleted_id": comment_id})

;     except Exception as e:
;         return jsonify({"status": "error", "error": str(e)}), 500





; @app.route("/detect-risk", methods=["POST"])
; def risk_only():
;     data    = request.get_json() or {}
;     comment = data.get("komentar", "")
;     cleaned = clean_text(comment)
;     return jsonify({"risk_flag": detect_risk(cleaned)})

; if __name__ == "__main__":
;     app.run(host="0.0.0.0", port=5000, debug=True)
